{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4a019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from shutil import copyfile,rmtree\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a78575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22424 entries, 0 to 22423\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   classname  22424 non-null  object\n",
      " 1   img        22424 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 350.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(classname       10\n",
       " img          22424\n",
       " dtype: int64,\n",
       " None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the list of all train images\n",
    "data = pd.read_csv(\"C://Users//almuhyaru//Downloads//state-farm-distracted-driver-detection//driver_imgs_list.csv\", usecols = [1,2])\n",
    "data.nunique(),data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7925c19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of all classes\n",
    "classes_list = data['classname'].unique()\n",
    "classes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "827b1d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0 : 2489\n",
      "c1 : 2267\n",
      "c2 : 2317\n",
      "c3 : 2346\n",
      "c4 : 2326\n",
      "c5 : 2312\n",
      "c6 : 2325\n",
      "c7 : 2002\n",
      "c8 : 1911\n",
      "c9 : 2129\n"
     ]
    }
   ],
   "source": [
    "#dictionary containing all train data file names, class wise\n",
    "train_data_files={}\n",
    "for cls, image_name in data.values:\n",
    "    key = cls\n",
    "    if key in train_data_files:\n",
    "        train_data_files[key].append(image_name)\n",
    "    else:\n",
    "        train_data_files[key] = [image_name]\n",
    "\n",
    "# printing the size of dataset for each class\n",
    "for key in train_data_files:\n",
    "    print(key, \":\", len(train_data_files[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5edb57",
   "metadata": {},
   "source": [
    "The 10 classes to predict are:\n",
    "\n",
    "c0: normal driving\n",
    "c1: texting - right\n",
    "c2: talking on the phone - right\n",
    "c3: texting - left\n",
    "c4: talking on the phone - left\n",
    "c5: operating the radio\n",
    "c6: drinking\n",
    "c7: reaching behind\n",
    "c8: hair and makeup\n",
    "c9: talking to passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67630b77",
   "metadata": {},
   "source": [
    "### Splittin, transofrming and generating image data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1874edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade4d730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17943 images belonging to 10 classes.\n",
      "Found 4481 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR='C://Users//almuhyaru//Downloads//state-farm-distracted-driver-detection/imgs/train'\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        validation_split = 0.2\n",
    ")\n",
    "\n",
    "training_data = datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        subset='training',shuffle=False)\n",
    "\n",
    "evaluating_data = datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        subset='validation',shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565d0ad",
   "metadata": {},
   "source": [
    "### Creating the model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe557a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6026f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "      Conv2D(16, (3,3), activation='relu', input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "      MaxPooling2D(2, 2),\n",
    "      Conv2D(32, (3,3), activation='relu'),\n",
    "      MaxPooling2D(2, 2),\n",
    "      Conv2D(64, (3,3), activation='relu'),\n",
    "      MaxPooling2D(2, 2),\n",
    "      Flatten(),\n",
    "      Dense(1024, activation='relu'),\n",
    "      Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c79a393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 16)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 43264)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              44303360  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44337194 (169.13 MB)\n",
      "Trainable params: 44337194 (169.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer= Adam(learning_rate = 0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b59e7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DELTA=0.005\n",
    "EPOCHS=20\n",
    "PATIENCE=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b7d09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop training if no significant change in validation data accuracy\n",
    "es = EarlyStopping(monitor = 'val_accuracy', patience = PATIENCE, min_delta = MIN_DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fd2422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "141/141 [==============================] - 492s 3s/step - loss: 2.7349 - accuracy: 0.0589 - val_loss: 2.2991 - val_accuracy: 0.1457\n",
      "Epoch 2/20\n",
      "141/141 [==============================] - 381s 3s/step - loss: 2.3239 - accuracy: 0.1078 - val_loss: 2.2817 - val_accuracy: 0.2513\n",
      "Epoch 3/20\n",
      "141/141 [==============================] - 374s 3s/step - loss: 1.7402 - accuracy: 0.4089 - val_loss: 1.1950 - val_accuracy: 0.6050\n",
      "Epoch 4/20\n",
      "141/141 [==============================] - 785s 6s/step - loss: 0.3705 - accuracy: 0.8918 - val_loss: 0.0998 - val_accuracy: 0.9737\n",
      "Epoch 5/20\n",
      "141/141 [==============================] - 1436s 10s/step - loss: 0.0504 - accuracy: 0.9876 - val_loss: 0.0527 - val_accuracy: 0.9877\n",
      "Epoch 6/20\n",
      "141/141 [==============================] - 460s 3s/step - loss: 0.0215 - accuracy: 0.9943 - val_loss: 0.0276 - val_accuracy: 0.9940\n",
      "Epoch 7/20\n",
      "141/141 [==============================] - 463s 3s/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0259 - val_accuracy: 0.9946\n",
      "Epoch 8/20\n",
      "141/141 [==============================] - 467s 3s/step - loss: 7.4153e-04 - accuracy: 0.9999 - val_loss: 0.0273 - val_accuracy: 0.9942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2234ab48fa0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting and generating the model\n",
    "model.fit(\n",
    "    training_data, \n",
    "    epochs = EPOCHS, \n",
    "    validation_data = evaluating_data,\n",
    "    callbacks = [es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858b8f7",
   "metadata": {},
   "source": [
    "### Predict and load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2e4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8311600",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir=\"C://Users//almuhyaru//Downloads//state-farm-distracted-driver-detection/imgs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63618623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# transforming test data same as training data\n",
    "test_data_gen = ImageDataGenerator(\n",
    "    rescale = 1./225\n",
    ")\n",
    "\n",
    "test_data = test_data_gen.flow_from_directory(img_dir,\n",
    "                          target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "                          classes = ['test'],\n",
    "                          shuffle = False,\n",
    "                          batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3aebf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 1035s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(79726, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(test_data)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c308e",
   "metadata": {},
   "source": [
    "###  Loading the prediction in required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9a943a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_files = image_dataset_from_directory(\n",
    "    'C://Users//almuhyaru//Downloads//state-farm-distracted-driver-detection/imgs/test',\n",
    "     labels = None,\n",
    "    label_mode=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8af097c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predicted)\n",
    "df.columns = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
    "filepath = [i.split('/')[-1] for i in test_data_files.file_paths]\n",
    "df1 = pd.DataFrame(filepath)\n",
    "df1.columns = ['img']\n",
    "df = df1.join(df)\n",
    "df.to_csv('output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "059aaa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79726 entries, 0 to 79725\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   img     79726 non-null  object \n",
      " 1   c0      79726 non-null  float32\n",
      " 2   c1      79726 non-null  float32\n",
      " 3   c2      79726 non-null  float32\n",
      " 4   c3      79726 non-null  float32\n",
      " 5   c4      79726 non-null  float32\n",
      " 6   c5      79726 non-null  float32\n",
      " 7   c6      79726 non-null  float32\n",
      " 8   c7      79726 non-null  float32\n",
      " 9   c8      79726 non-null  float32\n",
      " 10  c9      79726 non-null  float32\n",
      "dtypes: float32(10), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbab83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
